{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code with the modified sampeling period case 4 considering time delay at output\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle\n",
    "import csv\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# problem = \"Pendulum-v1\"\n",
    "# env = gym.make(problem)\n",
    "\n",
    "# num_states = env.observation_space.shape[0]\n",
    "# print(\"Size of State Space ->  {}\".format(num_states))\n",
    "# num_actions = env.action_space.shape[0]\n",
    "# print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "# upper_bound = env.action_space.high[0]\n",
    "# lower_bound = env.action_space.low[0]\n",
    "\n",
    "# print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "# print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the environment\n",
    "\n",
    "num_states=13 # 3: Considering z pos, z vel, z acc; 4: pos,vel,acc,u ; 8: pos, vel, acc, p_pos, p_vel, p_acc, u, p_u\n",
    "\n",
    "num_actions=1 # just controling u_z\n",
    "\n",
    "num_measurements=3 # For time delayed systems\n",
    "\n",
    "mass=0.67 #in kg\n",
    "\n",
    "g=9.81\n",
    "#upper_bound=5\n",
    "#lower_bound=-5\n",
    "upper_bound= mass*g # i.e. coefficient of thrust to weight=2\n",
    "\n",
    "lower_bound= -mass * g # we cannot do better without upside-down\n",
    "\n",
    "output_length=12\n",
    "\n",
    "vel_reward_weight_factor=0.1\n",
    "\n",
    "# lib_interface stuff\n",
    "import glob\n",
    "import ctypes\n",
    "import math\n",
    "\n",
    "libfile1 = glob.glob('../Sim_intfc/out/build/linux-default/libinterface_simulink.so.1')[0]\n",
    "mylib1 = ctypes.CDLL(libfile1)\n",
    "\n",
    "libfile2 = glob.glob('../Sim_intfc/out/build/linux-default/libinterface_simulink.so (copy).1')[0]\n",
    "mylib2 = ctypes.CDLL(libfile2)\n",
    "\n",
    "libfile3 = glob.glob('../Sim_intfc/out/build/linux-default/libinterface_simulink.so (another copy).1')[0]\n",
    "mylib3 = ctypes.CDLL(libfile3)\n",
    "\n",
    "libfile4 = glob.glob('../Sim_intfc/out/build/linux-default/libinterface_simulink.so (3rd copy).1')[0]\n",
    "mylib4 = ctypes.CDLL(libfile4)\n",
    "\n",
    "class EnvUAV_cmd_z:\n",
    "    simulink_input_length=1\n",
    "\n",
    "    simulink_states_length=28\n",
    "    print_period=1 #in seconds\n",
    "    episode_timeout=10 #in seconds\n",
    "    terminal_states=[0,0,0]\n",
    "    termination_causes={'reached_goal':0,'time_out':1,'nan_error':2}\n",
    "    def __init__(self,simulation_dt,sampling_dt,termination_cost,mylib,mass=1,CTW=2,g=9.81):\n",
    "        self.mylib = mylib\n",
    "        self.mylib.LoadModel()\n",
    "        self.mylib.run_a_step.restype = np.ctypeslib.ndpointer(dtype=np.float64,shape=(self.simulink_states_length,))\n",
    "        self.mylib.run_a_step.argtypes = [np.ctypeslib.ndpointer(dtype=np.float64),ctypes.c_int]\n",
    "        #self.mass=mass\n",
    "        #self.CTW=CTW\n",
    "        #self.g=g\n",
    "\n",
    "        self.simulation_dt=simulation_dt\n",
    "        self.sampling_dt=sampling_dt\n",
    "        self.sim_steps_per_samp_period=round(self.sampling_dt/self.simulation_dt) # Note sim dt must not be larger than sampling dt\n",
    "\n",
    "        self.steps_per_print_period=int(EnvUAV_cmd_z.print_period/self.sampling_dt)\n",
    "        self.sampling_step_counter=0\n",
    "        self.reset()\n",
    "    \n",
    "    def __call__(self,input):\n",
    "        #print(\"norhan\")\n",
    "        self.sampling_step_counter=self.sampling_step_counter+1\n",
    "        inputs = np.zeros(EnvUAV_cmd_z.simulink_input_length,dtype=np.float64)\n",
    "        inputs[0]=np.copy(input[0])\n",
    "        for i in range(self.sim_steps_per_samp_period): # Simulink solver is run at a higher rate for numerical stability\n",
    "            self.states_meas = self.mylib.run_a_step(inputs,len(inputs))\n",
    "        #terminal_states=[self.states_meas[25],0,0]   \n",
    "        self.z_states_meas[0]=np.copy(self.states_meas[2])-self.states_meas[25]\n",
    "        #print(self.z_states_meas[0])\n",
    "        self.z_states_meas[1]=np.copy(self.states_meas[5])-self.states_meas[26]\n",
    "        if num_states>2:\n",
    "            self.z_states_meas[2]=np.copy(self.states_meas[8])-self.states_meas[27]\n",
    "        if num_states==4:\n",
    "            self.z_states_meas[3]=np.copy(input[0])\n",
    "            self.prev_input=np.copy(input[0])\n",
    "            self.prev_measurements=np.copy(self.z_states_meas[0:3])\n",
    "        if num_states==7: #shehata suggestion d=4\n",
    "            #self.z_states_meas[3:39]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[3]=np.copy(input[0])\n",
    "            self.z_states_meas[4:13]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:3] = self.prevv_input[0:2]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "        if num_states==9: #shehata suggestion d=6\n",
    "            #self.z_states_meas[3:39]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[3]=np.copy(input[0])\n",
    "            self.z_states_meas[4:9]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:8] = self.prevv_input[0:7]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "        if num_states==19: #d=4\n",
    "            self.z_states_meas[3:15]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[15]=np.copy(input[0])\n",
    "            self.z_states_meas[16:19]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:3] = self.prevv_input[0:2]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "            self.prevv_measurements=np.copy(self.z_states_meas[0:12])\n",
    "            #print(self.z_states_meas)\n",
    "            \n",
    "        if num_states==23: #d=5\n",
    "            self.z_states_meas[3:18]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[18]=np.copy(input[0])\n",
    "            self.z_states_meas[19:23]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:4] = self.prevv_input[0:3]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "            self.prevv_measurements=np.copy(self.z_states_meas[0:15])\n",
    "            \n",
    "        if num_states==27: #d=6\n",
    "            self.z_states_meas[3:21]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[21]=np.copy(input[0])\n",
    "            self.z_states_meas[22:27]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:5] = self.prevv_input[0:4]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "            self.prevv_measurements=np.copy(self.z_states_meas[0:18])\n",
    "        if num_states==43: #d=10\n",
    "            self.z_states_meas[3:33]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[33]=np.copy(input[0])\n",
    "            self.z_states_meas[34:43]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:9] = self.prevv_input[0:8]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "            self.prevv_measurements=np.copy(self.z_states_meas[0:30])\n",
    "        if num_states==15: #shehata suggestion\n",
    "            #self.z_states_meas[3:39]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[3]=np.copy(input[0])\n",
    "            self.z_states_meas[4:15]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:11] = self.prevv_input[0:10]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "            #self.prev_measurements=np.copy(self.z_states_meas[0:36])\n",
    "            #print(self.z_states_meas)\n",
    "        if num_states==13: #shehata suggestion d=10\n",
    "            #self.z_states_meas[3:39]=np.copy(self.prevv_measurements)\n",
    "            self.z_states_meas[3]=np.copy(input[0])\n",
    "            self.z_states_meas[4:13]=np.copy(self.prevv_input)\n",
    "            self.prevv_input[1:9] = self.prevv_input[0:8]\n",
    "            self.prevv_input[0]=np.copy(input[0])\n",
    "\n",
    "        #cost=np.linalg.norm(np.array([self.z_states_meas[0],self.z_states_meas[1],self.z_states_meas[2]]))\n",
    "        #terminal_states=[self.states_meas[25],0,0]\n",
    "        #print(self.states_meas[25])\n",
    "        cost=np.linalg.norm(np.array([self.z_states_meas[0],vel_reward_weight_factor*self.z_states_meas[1]]))\n",
    "        self.reward=-cost*self.sampling_dt*100 # By 100 to 25 up reward for training\n",
    "        if en_goal_termination==True:    \n",
    "            if (cost<termination_cost):\n",
    "                self.termination_cause=EnvUAV_cmd_z.termination_causes['reached_goal']\n",
    "                print('Reached Goal!')\n",
    "                self.done=True\n",
    "        #print(self.sampling_step_counter)\n",
    "        #print(self.sampling_step_counter*self.sampling_dt>EnvUAV_cmd_z.episode_timeout)\n",
    "        #print(self.sampling_step_counter*self.sampling_dt)\n",
    "        if (self.sampling_step_counter*self.sampling_dt>EnvUAV_cmd_z.episode_timeout):\n",
    "            self.termination_cause=EnvUAV_cmd_z.termination_causes['time_out']\n",
    "            #print(self.sampling_step_counter,self.sampling_dt,EnvUAV_cmd_z.episode_timeout)\n",
    "            self.done=True\n",
    "        \n",
    "        if (math.isnan(self.z_states_meas[0]) or math.isnan(self.z_states_meas[1])):\n",
    "            self.z_states_meas=[0,0,0] # This is incorrect, but hopefully it is waived by stochastic learning\n",
    "            self.reward=np.ones(1)*0.0\n",
    "            self.termination_cause=EnvUAV_cmd_z.termination_causes['nan_error']\n",
    "            print('Warning NaN termination')\n",
    "            self.done=True\n",
    "        return self.z_states_meas, self.reward, self.done, self.termination_cause, self.states_meas[25], self.states_meas[2]\n",
    "\n",
    "    def reset(self):\n",
    "        if self.sampling_step_counter>0:\n",
    "            self.mylib.TermModel()\n",
    "        self.sampling_step_counter=0\n",
    "        self.mylib.LoadModel()\n",
    "        self.mylib.InitModel()\n",
    "        self.prev_measurements=np.zeros(num_measurements)\n",
    "        self.prevv_measurements=np.zeros(18)\n",
    "        self.done=False\n",
    "        self.z_states_meas=np.zeros(num_states)\n",
    "        self.prev_input=0.0\n",
    "        self.prevv_input=np.zeros(9)\n",
    "        self.termination_cause=EnvUAV_cmd_z.termination_causes['reached_goal']\n",
    "        return self.z_states_meas\n",
    "\n",
    "    def print_states_periodic(self):\n",
    "        if self.sampling_step_counter % self.steps_per_print_period ==1:\n",
    "            print('z pos: ' + str(self.states_meas[2]))\n",
    "            print('z vel: ' + str(self.states_meas[5]))\n",
    "            print('simulation time (s): ' + str(self.sampling_step_counter*self.sampling_dt))\n",
    "            print('===================')\n",
    "\n",
    "termination_cost=0.5 # e.g. 0.05 is 5cm , 5cm/s , 5cm/s^2\n",
    "simulink_dt=0.001 # Simulink solver step time\n",
    "sampling_dt=0.06 # The one that would be used experimentally. e.g. 0.005 corresponds to 200Hz\n",
    "UAV_alt_ctrl=EnvUAV_cmd_z(simulink_dt,sampling_dt,termination_cost, mylib1)\n",
    "UAV_alt_ctrl2=EnvUAV_cmd_z(simulink_dt,sampling_dt,termination_cost, mylib2)\n",
    "UAV_alt_ctrl3=EnvUAV_cmd_z(simulink_dt,sampling_dt,termination_cost, mylib3)\n",
    "UAV_alt_ctrl4=EnvUAV_cmd_z(simulink_dt,sampling_dt,termination_cost, mylib4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(UAV_alt_ctrl.sim_steps_per_samp_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self,target_actor,target_critic,actor_model, critic_model,gamma, critic_optimizerr,actor_optimizerr, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "        self.critic_optimizerr = critic_optimizerr\n",
    "        self.actor_optimizerr = actor_optimizerr\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "        self.gamma = gamma\n",
    "        self.target_actor = target_actor\n",
    "        self.target_critic = target_critic\n",
    "        self.actor_model = actor_model\n",
    "        self.critic_model = critic_model\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        #print(\"ready\")\n",
    "        with tf.GradientTape() as tape:\n",
    "            #print(\"steady\")\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            #print(\"steady0\")\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            #print(\"steady\")\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            #print(\"steady2\")\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "        #print(\"steady\")\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        #print(critic_grad)\n",
    "        #print(self.critic_model.trainable_variables)\n",
    "        #print(\"steadyyayay\")\n",
    "        #print(zip(critic_grad, self.critic_model.trainable_variables))\n",
    "        self.critic_optimizerr.apply_gradients(zip(critic_grad, self.critic_model.trainable_variables))\n",
    "        #print(\"steadyyay\")\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "        #print(\"go\")\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizerr.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "        #print(\"yay\")\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        #print(\"ready\")\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        #print(\"steady\")\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        #print(\"go\")\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        #print(\"yay\")\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        #print(\"yay\")\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau=0.005):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * 0.005 + a * (1 - 0.005))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(scale_actor):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.2, maxval=0.2) #MC\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(scale_actor*512, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(scale_actor*512, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic(scale_critic):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(scale_critic*32, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(scale_critic*64, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(scale_critic*64, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(scale_critic*512, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(scale_critic*512, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(actor_model,state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise # WARNING NOISE MIGHT BE DISABLED\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL models\n",
    "actor_model = get_actor(1)\n",
    "critic_model = get_critic(1)\n",
    "\n",
    "target_actor = get_actor(1)\n",
    "target_critic = get_critic(1)\n",
    "#####\n",
    "actor_model2 = get_actor(1)\n",
    "critic_model2 = get_critic(1)\n",
    "\n",
    "target_actor2 = get_actor(1)\n",
    "target_critic2 = get_critic(1)\n",
    "#####\n",
    "actor_model3 = get_actor(1)\n",
    "critic_model3 = get_critic(1)\n",
    "\n",
    "target_actor3 = get_actor(1)\n",
    "target_critic3 = get_critic(1)\n",
    "#####\n",
    "actor_model4 = get_actor(1)\n",
    "critic_model4 = get_critic(1)\n",
    "\n",
    "target_actor4 = get_actor(1)\n",
    "target_critic4 = get_critic(1)\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "target_actor2.set_weights(actor_model2.get_weights())\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "target_actor3.set_weights(actor_model3.get_weights())\n",
    "target_critic3.set_weights(critic_model3.get_weights())\n",
    "\n",
    "target_actor4.set_weights(actor_model4.get_weights())\n",
    "target_critic4.set_weights(critic_model4.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "####\n",
    "critic_lr2 = 0.002\n",
    "actor_lr2 = 0.001\n",
    "####\n",
    "critic_lr3 = 0.002\n",
    "actor_lr3 = 0.001\n",
    "####\n",
    "critic_lr4 = 0.002\n",
    "actor_lr4 = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "#####\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(critic_lr2)\n",
    "actor_optimizer2 = tf.keras.optimizers.Adam(actor_lr2)\n",
    "#####\n",
    "critic_optimizer3 = tf.keras.optimizers.Adam(critic_lr3)\n",
    "actor_optimizer3 = tf.keras.optimizers.Adam(actor_lr3)\n",
    "#####\n",
    "critic_optimizer4 = tf.keras.optimizers.Adam(critic_lr4)\n",
    "actor_optimizer4 = tf.keras.optimizers.Adam(actor_lr4)\n",
    "\n",
    "total_episodes = 5000\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(target_actor,target_critic,actor_model, critic_model, gamma, critic_optimizer,actor_optimizer,200000, 1024)\n",
    "##\n",
    "buffer2 = Buffer(target_actor2,target_critic2,actor_model2, critic_model2, gamma, critic_optimizer2,actor_optimizer2,200000, 1024)\n",
    "##\n",
    "buffer3 = Buffer(target_actor3,target_critic3,actor_model3, critic_model3, gamma, critic_optimizer3,actor_optimizer3,200000, 1024)\n",
    "##\n",
    "buffer4 = Buffer(target_actor4,target_critic4,actor_model4, critic_model4, gamma, critic_optimizer4,actor_optimizer4,200000, 1024)\n",
    "\n",
    "\n",
    "# nosie model\n",
    "init_std_dev = 0.4\n",
    "final_exponent_val=10 #formula std_dev=init_std_dev * exp^(-(ep/total_episodes)*final_exponent_val)\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(init_std_dev) * np.ones(1))\n",
    "en_noise_decay=False\n",
    "\n",
    "# model behaviour\n",
    "en_goal_termination=False\n",
    "\n",
    "train_inputs = [\n",
    "        (UAV_alt_ctrl, buffer, actor_model, critic_model, target_actor, target_critic, critic_lr, actor_lr),\n",
    "        #(UAV_alt_ctrl2, buffer2, actor_model2, critic_model2, target_actor2, target_critic2, critic_lr2, actor_lr2),\n",
    "        #(UAV_alt_ctrl3, buffer3, actor_model3, critic_model3, target_actor3, target_critic3, critic_lr3, actor_lr3),\n",
    "        #(UAV_alt_ctrl4, buffer4, actor_model4, critic_model4, target_actor4, target_critic4, critic_lr4, actor_lr4)\n",
    "    ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(indexx, UAV_alt_ctrll, bufferr, actor_modell, critic_modell, target_actorr, target_criticc, critic_lrr, actor_lrr):\n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "\n",
    "    # To store average reward history of the last few episodes\n",
    "    avg_reward_list = []\n",
    "\n",
    "    # To store final states of the system\n",
    "    final_states_reward_list = []  # [states, reward]\n",
    "\n",
    "    # To store termination status\n",
    "    termination_causes_list = []\n",
    "\n",
    "    # To store all state_action pairs\n",
    "    states_action_list = []\n",
    "    states_action_lists = []\n",
    "    window_state_list = []\n",
    "    window_state_lists = []\n",
    "    z_state_list = []\n",
    "    z_state_lists = []\n",
    "\n",
    "    learning_frequency = 256  # MC\n",
    "\n",
    "    for ep in range(total_episodes):\n",
    "        # prev_state = env.reset()\n",
    "        prev_state = UAV_alt_ctrll.reset()\n",
    "        episodic_reward = 0  # Total rewards in one episode\n",
    "        #print(prev_state)\n",
    "        while True:\n",
    "            # Uncomment this to see the Actor in action\n",
    "            # But not in a python notebook.\n",
    "            # env.render()\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)  # Necessary for tensorflow (check immutable/mutable data types if you want to know more)\n",
    "            #print(tf_prev_state)\n",
    "            action = policy(actor_modell,tf_prev_state, ou_noise)\n",
    "            # Receive state and reward from the environment.\n",
    "            # state, reward, done, info = env.step(action)\n",
    "            #print(\"before execution\")\n",
    "            #print(action)\n",
    "            state, reward, done, termination_cause, window_state, z_position = UAV_alt_ctrll(action)\n",
    "            #print(\"after execution\")\n",
    "            states_action_list.append([state.copy(), action[0].copy()])\n",
    "            #print(\"1\")\n",
    "            window_state_list.append(window_state.copy())\n",
    "            #print(\"2\")\n",
    "            z_state_list.append(z_position.copy())\n",
    "            #print(\"3\")\n",
    "            # UAV_alt_ctrl.print_states_periodic()\n",
    "\n",
    "            buffer.record((prev_state.copy(), action.copy(), reward.copy(), state.copy()))\n",
    "            episodic_reward += reward\n",
    "            #print(\"4\")\n",
    "            if buffer.buffer_counter > buffer.batch_size:\n",
    "                #print(\"6\")\n",
    "                if (buffer.buffer_counter % (int(buffer.batch_size / learning_frequency)) == 1):\n",
    "                    #print(\"7raaaaaaaam\")\n",
    "                    buffer.learn()\n",
    "                    print(\"learn\")\n",
    "                    update_target(target_actorr.variables, actor_modell.variables, 0.005)\n",
    "                    update_target(target_criticc.variables, critic_modell.variables, 0.005)\n",
    "            #print(\"5\")\n",
    "            # End this episode when `done` is True\n",
    "            \n",
    "            #print(\"before done\")\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = np.copy(state)\n",
    "            #print(prev_state)\n",
    "            \n",
    "        #print(\"outside while\")\n",
    "        ep_reward_list.append(episodic_reward)\n",
    "        termination_causes_list.append(termination_cause)\n",
    "        final_states_reward_list.append([state.copy(), reward.copy()])\n",
    "        states_action_lists.append(states_action_list.copy())\n",
    "        window_state_lists.append(window_state_list.copy())\n",
    "        z_state_lists.append(z_state_list.copy())\n",
    "        states_action_list.clear()\n",
    "        window_state_list.clear()\n",
    "        z_state_list.clear()\n",
    "\n",
    "        # Mean of the last 40 episodes\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        print(\"Episode * {} * Avg Reward is ==> {} from process {}\".format(ep, avg_reward,indexx))\n",
    "        print(\"Episode * {} * Reward is ==> {}\".format(ep, episodic_reward))\n",
    "\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        if en_noise_decay:\n",
    "            ou_noise.std_dev[0] = init_std_dev * math.exp(-(ep / total_episodes) * final_exponent_val)\n",
    "    return termination_causes_list, z_state_lists, window_state_lists, states_action_lists, ep_reward_list, avg_reward_list\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#termination_causes_list, z_state_lists, window_state_lists, states_action_lists, ep_reward_list, avg_reward_list = train(1, UAV_alt_ctrl, buffer, actor_model, critic_model, target_actor, target_critic, critic_lr, actor_lr)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "class TrainingProcess(multiprocessing.Process):\n",
    "    def __init__(self, input_args, results_queue):\n",
    "        super().__init__()\n",
    "        self.input_args = input_args\n",
    "        self.results_queue = results_queue\n",
    "\n",
    "    def run(self):\n",
    "        indexx, UAV_alt_ctrl, buffer, actor_model, critic_model, target_actor, target_critic, critic_lr, actor_lr = self.input_args\n",
    "\n",
    "        # Create a separate StringIO for capturing stdout\n",
    "        stdout_capture = io.StringIO()\n",
    "\n",
    "        #with redirect_stdout(stdout_capture):\n",
    "        termination_causes_list, z_state_lists, window_state_lists, states_action_lists, ep_reward_list, avg_reward_list = train(indexx,UAV_alt_ctrl, buffer, actor_model, critic_model, target_actor, target_critic, critic_lr, actor_lr)\n",
    "\n",
    "            \n",
    "        # Store the results in a tuple\n",
    "        results = (termination_causes_list, z_state_lists, window_state_lists, states_action_lists, ep_reward_list, avg_reward_list)  # Corrected to use the calculated result\n",
    "            \n",
    "        # Put the results into the queue\n",
    "        self.results_queue.put(results)    \n",
    "            \n",
    "            \n",
    "results_queue = multiprocessing.Queue()\n",
    "\n",
    "# Create and start separate processes for each training\n",
    "processes = []\n",
    "for i, input_args in enumerate(train_inputs):\n",
    "    input_args = (i,) + input_args\n",
    "    process = TrainingProcess(input_args, results_queue)\n",
    "    processes.append(process)\n",
    "    print(f\"Starting process: {i}\")\n",
    "    #print(\"norhan\")\n",
    "    process.start()\n",
    "    #print(\"norhan\")\n",
    "#print(\"norhan\")\n",
    "# # Wait for all processes to finish\n",
    "for process in processes:\n",
    "    process.join()\n",
    "\n",
    "print(\"processes joined\")\n",
    "# Collect results from the queue\n",
    "results_list = [results_queue.get() for _ in range(len(train_inputs))]\n",
    "# Access the results from the results_list for each process\n",
    "process1_results = results_list[0]\n",
    "process2_results = results_list[1]\n",
    "process3_results = results_list[2]\n",
    "process4_results = results_list[3]\n",
    "\n",
    "termination_causes_list, z_state_lists, window_state_lists, states_action_lists, ep_reward_list, avg_reward_list = process1_results\n",
    "termination_causes_list_2, z_state_lists_2, window_state_lists_2, states_action_lists_2, ep_reward_list_2, avg_reward_list_2 = process2_results\n",
    "termination_causes_list_3, z_state_lists_3, window_state_lists_3, states_action_lists_3, ep_reward_list_3, avg_reward_list_3 = process3_results\n",
    "termination_causes_list_4, z_state_lists_4, window_state_lists_4, states_action_lists_4, ep_reward_list_4, avg_reward_list_4 = process4_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(termination_causes_list)\n",
    "print(processes)\n",
    "print(termination_causes_list,termination_causes_list_2,termination_causes_list_3,termination_causes_list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "num_of_episodes=len(avg_reward_list)\n",
    "t=np.arange(num_of_episodes)+(len(avg_reward_list)-num_of_episodes)\n",
    "plt.plot(t,avg_reward_list[-num_of_episodes:])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "plt.plot(t,ep_reward_list[-num_of_episodes:])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting states\n",
    "index_of_episode=150\n",
    "pos_vals=[]\n",
    "vel_vals=[]\n",
    "acc_vals=[]\n",
    "act_vals=[]\n",
    "window_vals=[]\n",
    "z_vals=[]\n",
    "inst_reward=[]\n",
    "accum_reward=[]\n",
    "time_vals=np.linspace(0,EnvUAV_cmd_z.episode_timeout,len(states_action_lists[index_of_episode]))\n",
    "\n",
    "for tuble in states_action_lists[index_of_episode]:\n",
    "    pos_vals.append(tuble[0][0])\n",
    "    vel_vals.append(tuble[0][1])\n",
    "    acc_vals.append(tuble[0][2])\n",
    "    act_vals.append(tuble[1])\n",
    "    inst_reward.append(-np.linalg.norm(np.array([pos_vals[-1],vel_reward_weight_factor*vel_vals[-1]]))*UAV_alt_ctrl.sampling_dt*100*0.1)\n",
    "    accum_reward.append(sum(inst_reward))\n",
    "\n",
    "window_vals = window_state_lists[index_of_episode]\n",
    "z_vals = z_state_lists[index_of_episode]\n",
    "\n",
    "plt.close()\n",
    "figure(figsize=(24, 10), dpi=80)\n",
    "\n",
    "plt.plot(time_vals,pos_vals)\n",
    "plt.plot(time_vals,vel_vals)\n",
    "plt.plot(time_vals,acc_vals)\n",
    "plt.plot(time_vals,act_vals)\n",
    "#plt.plot(time_vals,accum_reward)\n",
    "#plt.plot(time_vals,inst_reward)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"State and Action Trajectories\")\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.legend(['Position','Velocity','Acceleration','Actor','accumelated reward (x0.1)','Instantaneous reward'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "time = time_vals\n",
    "altitude = z_vals\n",
    "\n",
    "# Sample window position vs. time data (replace with your data)\n",
    "window_time = time_vals\n",
    "window_position = window_vals\n",
    "\n",
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_vals,altitude)\n",
    "plt.plot(time_vals,window_position)\n",
    "ax.set_xlim(0, max(max(time), max(window_time)))\n",
    "ax.set_ylim(min(min(altitude), min(window_position)) - 1, max(max(altitude), max(window_position)) + 1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Altitude')\n",
    "plt.title('Drone Altitude and Window Altitude vs. Time')\n",
    "plt.legend(['Drone Altitude','Window Altitude'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(window_state_lists[index_of_episode])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Specify the CSV file name\n",
    "csv_file = 'time_data.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file, 'w') as file:\n",
    "    # Write the vector data to the file\n",
    "    for value in time_vals:\n",
    "        file.write(str(value) + '\\n')\n",
    "\n",
    "print(f\"Vector data saved to {csv_file} successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the CSV file name\n",
    "csv_file = 'pos_data.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file, 'w') as file:\n",
    "    # Write the vector data to the file\n",
    "    for value in pos_vals:\n",
    "        file.write(str(value) + '\\n')\n",
    "\n",
    "print(f\"Vector data saved to {csv_file} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the CSV file name\n",
    "csv_file = 'actor_data.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file, 'w') as file:\n",
    "    # Write the vector data to the file\n",
    "    for value in act_vals:\n",
    "        file.write(str(value) + '\\n')\n",
    "\n",
    "print(f\"Vector data saved to {csv_file} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of value function\n",
    "%matplotlib widget\n",
    "from random import sample\n",
    "from tabnanny import verbose\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "## Inquiry actor\n",
    "# Mesh initiation\n",
    "samples_per_dim=20\n",
    "\n",
    "pos_range=[-100,100]\n",
    "vel_range=[-100,100]\n",
    "acc_range=[-5,5]\n",
    "\n",
    "pos_states=np.linspace(pos_range[0],pos_range[1],samples_per_dim)\n",
    "\n",
    "vel_states=np.linspace(vel_range[0],vel_range[1],samples_per_dim)\n",
    "\n",
    "acc_states=np.linspace(acc_range[0],acc_range[1],samples_per_dim)\n",
    "\n",
    "value_function=np.linspace((pos_range[0],vel_range[0]),(pos_range[1],vel_range[1]),samples_per_dim)\n",
    "\n",
    "actor_output=np.zeros([samples_per_dim,samples_per_dim,samples_per_dim])\n",
    "\n",
    "value_function=np.ones([samples_per_dim,samples_per_dim,samples_per_dim])\n",
    "\n",
    "default_acc_state=acc_states[-1]\n",
    "\n",
    "for i in range(samples_per_dim):\n",
    "    for j in range(samples_per_dim):\n",
    "        #for k in range(samples_per_dim):\n",
    "        input_to_actor=np.asarray([pos_states[i],vel_states[j],default_acc_state])\n",
    "        input_to_actor=input_to_actor.reshape(1,3)\n",
    "        actor_output[i,j,0]=actor_model.predict(input_to_actor,verbose=0)\n",
    "\n",
    "\n",
    "for i in range(samples_per_dim):\n",
    "    for j in range(samples_per_dim):\n",
    "        #for k in range(samples_per_dim):\n",
    "        input_to_critic_states=np.asarray([pos_states[i],vel_states[j],default_acc_state])\n",
    "        input_to_critic_states=input_to_critic_states.reshape(1,3)  #norhan\n",
    "        input_to_critic_action=np.asarray([actor_output[i,j,0]])\n",
    "        value_function[i,j,0]=critic_model([input_to_critic_states,input_to_critic_action])\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating figure\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "  \n",
    "with open('value_function.csv','w') as f:\n",
    "    write = csv.writer(f)\n",
    "    for i in range(samples_per_dim):\n",
    "        for j in range(samples_per_dim):\n",
    "            ax.scatter(pos_states[i], vel_states[j], value_function[i,j,0], color='green')\n",
    "            #write_list=[pos_states[i],vel_states[j],value_function[i,j,0]]\n",
    "            #write.writerows(write_list)\n",
    "\n",
    "\n",
    "  \n",
    "# setting title and labels\n",
    "ax.set_title(\"3D plot\")\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Velocity')\n",
    "ax.set_zlabel('Value Function')\n",
    "  \n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actor output\n",
    "\n",
    "forced_acc_state=0.0\n",
    "num_pts=30\n",
    "\n",
    "actor_output=np.zeros(num_pts)\n",
    "\n",
    "# 1 fix velocity and vary position\n",
    "forced_velocity=0.0\n",
    "pos_max=10\n",
    "\n",
    "ranged_position=np.linspace(-pos_max,pos_max,num_pts)\n",
    "\n",
    "for i in range(num_pts):\n",
    "    input_to_actor=np.asarray([ranged_position[i],forced_velocity,forced_acc_state])\n",
    "    input_to_actor=input_to_actor.reshape(1,3)\n",
    "    actor_output[i]=actor_model.predict(input_to_actor,verbose=0)\n",
    "\n",
    "plt.close()\n",
    "fig=plt.figure(figsize=(24, 10), dpi=80)\n",
    "axes=fig.add_subplot(1,1,1)\n",
    "axes.set_xticks(np.arange(-pos_max,pos_max,1))\n",
    "plt.plot(ranged_position,actor_output)\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"action\")\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actor output\n",
    "\n",
    "# 2 fix position and vary velocity\n",
    "forced_position=0.0\n",
    "vel_max=10\n",
    "ranged_vel=np.linspace(-vel_max,vel_max,num_pts)\n",
    "\n",
    "for i in range(num_pts):\n",
    "    input_to_actor=np.asarray([forced_position,ranged_vel[i],forced_acc_state])\n",
    "    input_to_actor=input_to_actor.reshape(1,3)\n",
    "    actor_output[i]=actor_model.predict(input_to_actor,verbose=0)\n",
    "\n",
    "plt.close()\n",
    "fig=plt.figure(figsize=(24, 10), dpi=80)\n",
    "axes=fig.add_subplot(1,1,1)\n",
    "axes.set_xticks(np.arange(-vel_max,vel_max,1))\n",
    "plt.plot(ranged_vel,actor_output)\n",
    "plt.xlabel(\"vel\")\n",
    "plt.ylabel(\"action\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepared data for save in csv\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "index_of_episode=7\n",
    "flat_list=[]\n",
    "\n",
    "for tuble in states_action_lists[index_of_episode]:\n",
    "    flat_list.append([tuble[0].tolist()+[tuble[1].tolist()]][0])\n",
    "\n",
    "with open('episode_trajectories.csv','w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(flat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"uav_z_u_cmd_actor.h5\")\n",
    "critic_model.save_weights(\"uav_z_u_cmd_critic.h5\")\n",
    "actor_model.save('model2',save_format='tf')\n",
    "\n",
    "target_actor.save_weights(\"uav_z_u_cmd_target_actor.h5\")\n",
    "target_critic.save_weights(\"uav_z_u_cmd_target_critic.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import (\n",
    "    convert_variables_to_constants_v2,\n",
    ")\n",
    "\n",
    "model = actor_model\n",
    "# Create frozen graph\n",
    "x = tf.TensorSpec(model.input_shape, tf.float32, name=\"x\")\n",
    "concrete_function = tf.function(lambda x: model(x)).get_concrete_function(x)\n",
    "frozen_model = convert_variables_to_constants_v2(concrete_function)\n",
    "\n",
    "# Check input/output node name\n",
    "print(f\"{frozen_model.inputs=}\")\n",
    "print(f\"{frozen_model.outputs=}\")\n",
    "\n",
    "# Save the graph as protobuf format\n",
    "directory = \".\"\n",
    "tf.io.write_graph(frozen_model.graph, directory, \"model3.pb\", as_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(actor_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_to_csv(name,*argv):\n",
    "    with open(name,'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerows(argv)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "additional_training_episodes=400\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(additional_training_episodes):\n",
    "\n",
    "    #prev_state = env.reset()\n",
    "    prev_state = UAV_alt_ctrl.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        \n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        #state, reward, done, info = env.step(action)\n",
    "        state, reward, done, termination_cause=UAV_alt_ctrl(action)\n",
    "        states_action_list.append([state.copy(),action[0].copy()])\n",
    "        #UAV_alt_ctrl.print_states_periodic()\n",
    "\n",
    "        buffer.record((prev_state.copy(), action.copy(), reward.copy(), state.copy()))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        if buffer.buffer_counter>buffer.batch_size:\n",
    "            if (buffer.buffer_counter%(int(buffer.batch_size/learning_frequency))==1):\n",
    "                buffer.learn()\n",
    "                update_target(target_actor.variables, actor_model.variables, 0.005)\n",
    "                update_target(target_critic.variables, critic_model.variables, 0.005)\n",
    "\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = np.copy(state)\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    termination_causes_list.append(termination_cause)\n",
    "    final_states_reward_list.append([state.copy(),reward.copy()])\n",
    "    states_action_lists.append(states_action_list.copy())\n",
    "    states_action_list.clear()\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(ep, episodic_reward))\n",
    "    \n",
    "    avg_reward_list.append(avg_reward)\n",
    "    if en_noise_decay:\n",
    "        ou_noise.std_dev[0]=init_std_dev * math.exp(-(ep/total_episodes)*final_exponent_val)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_actor_model = get_actor()\n",
    "\n",
    "loaded_critic_model=get_critic()\n",
    "\n",
    "loaded_actor_model.load_weights(\"uav_z_u_cmd_actor.h5\")\n",
    "\n",
    "loaded_critic_model.load_weights(\"uav_z_u_cmd_critic.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_run_loaded_actor=True\n",
    "en_accelerometer_noise=False\n",
    "acc_noise_std_dev=1\n",
    "\n",
    "test_UAV_alt_ctrl=EnvUAV_cmd_z(simulink_dt,sampling_dt,termination_cost)\n",
    "\n",
    "test_time=5.0 # Seconds\n",
    "\n",
    "running_steps=int(test_time/sampling_dt)\n",
    "states=np.zeros(num_states)\n",
    "states=test_UAV_alt_ctrl.reset()\n",
    "\n",
    "states_history=np.zeros([running_steps,num_states])\n",
    "actions_history=np.zeros(running_steps)\n",
    "for i in range(running_steps):\n",
    "    if en_accelerometer_noise:\n",
    "        states[2]=states[2]+np.random.normal(0.0,acc_noise_std_dev)\n",
    "    input_to_actor=np.asarray(states)\n",
    "    input_to_actor=input_to_actor.reshape(1,num_states)\n",
    "    if en_run_loaded_actor:\n",
    "        actor_output=loaded_actor_model.predict(input_to_actor,verbose=0)\n",
    "    else:\n",
    "        actor_output=actor_model.predict(input_to_actor,verbose=0)\n",
    "    \n",
    "    states=test_UAV_alt_ctrl(actor_output)\n",
    "    states=states[0]\n",
    "    states_history[i]=np.copy(states)\n",
    "    actions_history[i]=np.copy(actor_output)\n",
    "\n",
    "time_vals=np.linspace(0,test_time,running_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "plt_range_time=test_time\n",
    "plotting_range=int((plt_range_time/test_time)*running_steps)\n",
    "\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig=plt.figure(figsize=(3, 5), dpi=160)\n",
    "axes=fig.add_subplot(1,1,1)\n",
    "axes.set_xticks(np.arange(0,test_time,1))\n",
    "\n",
    "plt.plot(time_vals[0:plotting_range],states_history[0:plotting_range,0], linewidth=0.75, label=r'Position ($m$)')\n",
    "plt.plot(time_vals[0:plotting_range],states_history[0:plotting_range,1], linewidth=0.75, label=r'Velocity ($\\frac{m}{s}$)')\n",
    "plt.plot(time_vals[0:plotting_range],states_history[0:plotting_range,2], linewidth=0.75, label=r'Acceleration ($\\frac{m}{s^2}$)')\n",
    "plt.plot(time_vals[0:plotting_range],actions_history[0:plotting_range], linewidth=0.75, label=r'Actor')\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"State Trajectories\")\n",
    "plt.legend(loc=4)\n",
    "plt.xlim([0, plt_range_time])\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(states_history)\n",
    "#print_to_csv('hi.csv',states_history[:,0],states_history[:,1],states_history[:,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing noise actions\n",
    "noise_tester=OUActionNoise(mean=np.zeros(1),std_deviation=np.ones(1)*0.04)\n",
    "\n",
    "list_generated_noise=[]\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    list_generated_noise.append(noise_tester())\n",
    "\n",
    "\n",
    "plt.close()\n",
    "\n",
    "plt.plot(list_generated_noise)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
